{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4401f29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3490ce54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prophet import Prophet\n",
    "from scipy.stats import weibull_min, norm, t\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nevergrad as ng\n",
    "import random\n",
    "from django.db import models\n",
    "import json\n",
    "import nlopt\n",
    "\n",
    "\n",
    "\n",
    "def bootci(samp, share_spend, boot_n, seed=1):\n",
    "    random.seed(seed)\n",
    "    n = len(samp)\n",
    "    ci_percentage = 1 - share_spend\n",
    "    ci_width = np.mean(samp) * ci_percentage\n",
    "    \n",
    "    # Se abbiamo un solo campione, non possiamo fare il bootstrap\n",
    "    if n <= 1:\n",
    "        return {'boot_means': np.array(samp), 'ci': (np.mean(samp) - ci_width, np.mean(samp) + ci_width), 'se': 0}\n",
    "    \n",
    "    samp_n = len(samp)\n",
    "    samp_mean = np.mean(samp)\n",
    "    boot_sample = np.random.choice(samp, size=(boot_n, samp_n), replace=True)\n",
    "    boot_means = np.mean(boot_sample, axis=1)\n",
    "    se = np.std(boot_means, ddof=1)\n",
    "\n",
    "    # Calculating the margin of error\n",
    "    me = t.ppf(0.975, samp_n - 1) * se\n",
    "    samp_me = me * np.sqrt(samp_n)\n",
    "    ci_low = samp_mean - samp_me\n",
    "    ci_up = samp_mean + samp_me\n",
    "    boot_mean = np.mean(boot_means)\n",
    "\n",
    "    if boot_mean == ci_low:\n",
    "        ci_low = samp_mean - ci_width\n",
    "    if boot_mean == ci_up:\n",
    "        ci_up = samp_mean + ci_width\n",
    "\n",
    "    ci = (ci_low, ci_up)\n",
    "\n",
    "    return {'boot_means': boot_means, 'ci': ci, 'se': se}\n",
    "\n",
    "def confidence_calcs(xDecompAgg, share_spend, all_paid, dep_var_type, k,\n",
    "                     boot_n=1000, sim_n=10000):\n",
    "    # Assuming the required preprocessing steps on xDecompAgg and cls are similar to those in the R code\n",
    "    df_clusters_outcome = xDecompAgg[xDecompAgg['total_spend'].notna()]\n",
    "    df_clusters_outcome = df_clusters_outcome[['solID', 'cluster', 'rn', 'roi_total', 'cpa_total']]\n",
    "    df_clusters_outcome = df_clusters_outcome[df_clusters_outcome['cluster'].notna()]\n",
    "    df_clusters_outcome = df_clusters_outcome.sort_values(by=['cluster', 'rn'])\n",
    "\n",
    "    cluster_collect = []\n",
    "    chn_collect = []\n",
    "    sim_collect = []\n",
    "\n",
    "    for j in k:\n",
    "        df_outcome = df_clusters_outcome[df_clusters_outcome['cluster'] == j]\n",
    "\n",
    "        if len(df_outcome['solID'].unique()) < 3:\n",
    "            print(f\"Warning: Cluster {j} does not contain enough models to calculate CI\")\n",
    "\n",
    "            for i in all_paid:\n",
    "\n",
    "                if dep_var_type in ['O', 'C', 'N']:\n",
    "                    df_chn = df_outcome[(df_outcome['rn'] == i) & np.isfinite(df_outcome['cpa_total'])]\n",
    "                    v_samp = df_chn['cpa_total'].values \n",
    "                else:\n",
    "                    df_chn = df_outcome[df_outcome['rn'] == i]\n",
    "                    v_samp = df_chn['roi_total'].values\n",
    "\n",
    "                if len(v_samp) > 0:\n",
    "                    boot_mean = np.mean(v_samp)\n",
    "\n",
    "                    df_chn = df_chn.copy()\n",
    "                    df_chn['ci_low'] = boot_mean\n",
    "                    df_chn['ci_up'] = boot_mean\n",
    "                    df_chn['n'] = len(v_samp)\n",
    "                    df_chn['boot_se'] = 0\n",
    "                    df_chn['boot_mean'] = boot_mean\n",
    "                    df_chn['cluster'] = j\n",
    "                    chn_collect.append(df_chn)\n",
    "\n",
    "                    x_sim = np.random.normal(boot_mean, 0, sim_n)\n",
    "                    y_sim = norm.pdf(x_sim, boot_mean, 0)\n",
    "                \n",
    "                else:\n",
    "                    boot_mean = 0\n",
    "                    x_sim = np.array([])\n",
    "                    y_sim = np.array([])\n",
    "\n",
    "                sim_collect.append(pd.DataFrame({'cluster': j, 'rn': i, 'n': len(v_samp),\n",
    "                                                    'boot_mean': boot_mean, 'x_sim': x_sim, 'y_sim': y_sim}))\n",
    "        else:\n",
    "            for i in all_paid:\n",
    "                if dep_var_type in ['O', 'C', 'N']:\n",
    "                    df_chn = df_outcome[(df_outcome['rn'] == i) & np.isfinite(df_outcome['cpa_total'])]\n",
    "                    v_samp = df_chn['cpa_total'].to_numpy()\n",
    "                else:\n",
    "                    df_chn = df_outcome[df_outcome['rn'] == i]\n",
    "                    v_samp = df_chn['roi_total'].to_numpy()  \n",
    "                \n",
    "                if len(v_samp) > 0:\n",
    "                    boot_res = bootci(v_samp, share_spend[i], boot_n=boot_n)\n",
    "                    boot_mean = np.mean(boot_res['boot_means'])\n",
    "                    boot_se = boot_res['se']\n",
    "                    ci_low = max(0, boot_res['ci'][0])\n",
    "                    ci_up = boot_res['ci'][1]\n",
    "\n",
    "                    # Collect loop results\n",
    "                    df_chn = df_chn.copy()\n",
    "                    df_chn['ci_low'] = ci_low\n",
    "                    df_chn['ci_up'] = ci_up\n",
    "                    df_chn['n'] = len(v_samp)\n",
    "                    df_chn['boot_se'] = boot_se\n",
    "                    df_chn['boot_mean'] = boot_mean\n",
    "                    df_chn['cluster'] = j\n",
    "                    chn_collect.append(df_chn)\n",
    "\n",
    "                    x_sim = np.random.normal(boot_mean, boot_se, sim_n)\n",
    "                    y_sim = norm.pdf(x_sim, boot_mean, boot_se)\n",
    "\n",
    "                else:\n",
    "                    boot_mean = 0\n",
    "                    x_sim = np.array([])\n",
    "                    y_sim = np.array([])\n",
    "                \n",
    "                sim_collect.append(pd.DataFrame({'cluster': j, 'rn': i, 'n': len(v_samp),\n",
    "                                                 'boot_mean': boot_mean, 'x_sim': x_sim, 'y_sim': y_sim}))\n",
    "                \n",
    "        #cluster_collect.append({'chn_collect': chn_collect, 'sim_collect': sim_collect})\n",
    "        cluster_collect.append({'chn_collect': chn_collect})\n",
    "\n",
    "    # Aggregating simulation data\n",
    "    '''sim_collect = pd.concat([pd.concat(x['sim_collect']) for x in cluster_collect])\n",
    "    sim_collect['cluster_title'] = sim_collect.apply(lambda row: f\"Cl.{row['cluster']} (n={row['n']})\", axis=1)'''\n",
    "\n",
    "    # Aggregating CI data\n",
    "    df_ci = pd.concat([pd.concat(x['chn_collect']) for x in cluster_collect])\n",
    "    df_ci['cluster_title'] = df_ci.apply(lambda row: f\"Cl.{row['cluster']} (n={row['n']})\", axis=1)\n",
    "    df_ci = df_ci.groupby(['rn', 'cluster_title', 'cluster']).agg({\n",
    "        'n': 'first', 'boot_mean': 'first', 'boot_se': 'first',\n",
    "        'ci_low': 'first', 'ci_up': 'first'\n",
    "    }).reset_index()\n",
    "\n",
    "    return {'df_ci': df_ci, 'sim_collect': sim_collect, 'boot_n': boot_n, 'sim_n': sim_n}\n",
    "\n",
    "\n",
    "def refresh_model(df, df_holidays, json_model, num_trials, iterations):\n",
    "    init_result, init_model, init_summary_dict, init_df_saturation_ridge, init_lambda_value, init_ridge_intercept, init_rsq, init_nrmse, init_rssd, init_mape = import_model(json_model, df, df_holidays, type_of_use='refresh')\n",
    "\n",
    "    def robyn_model_obj(**parameters):\n",
    "        new_summary_dict = {}\n",
    "        \n",
    "        for key, value in init_summary_dict.items():\n",
    "            if key + '_alphas' in parameters.keys():\n",
    "                new_summary_dict[key] = {'coef': parameters[key], 'alphas': parameters[key + '_alphas'], 'gammas': parameters[key + '_gammas'], 'shapes': parameters[key + '_shapes'], 'scales': parameters[key + '_scales']}\n",
    "            else:\n",
    "                new_summary_dict[key] = {'coef': parameters[key]}\n",
    "            \n",
    "        new_json_model = create_summary_and_hyper_values(json_model, new_summary_dict, init_lambda_value)\n",
    "\n",
    "        new_result, new_model, new_summary_dict_obj, new_df_saturation_ridge, new_lambda_value, new_ridge_intercept, new_rsq, new_nrmse, new_rssd, new_mape = import_model(new_json_model, df, df_holidays, type_of_use='refresh')\n",
    "        \n",
    "        if init_mape:\n",
    "            if new_mape == 0:\n",
    "                new_mape = 0.01\n",
    "            return [new_nrmse, new_rssd, new_mape]\n",
    "        else:\n",
    "            return [new_nrmse, new_rssd]\n",
    "        \n",
    "    date_var = json_model['InputCollect']['date_var'][0]\n",
    "    window_end = json_model['InputCollect']['window_end'][0]\n",
    "    window_start = json_model['InputCollect']['refreshAddedStart'][0]\n",
    "    \n",
    "    df_window = df.loc[(df[date_var] >= window_start) & (df[date_var] <= window_end)]\n",
    "    \n",
    "    paid_media_vars_and_signs = dict(zip(json_model['InputCollect']['paid_media_spends'], json_model['InputCollect']['paid_media_signs']))\n",
    "    if 'organic_vars' in json_model['InputCollect']:\n",
    "        organic_vars_and_signs = dict(zip(json_model['InputCollect']['organic_vars'], json_model['InputCollect']['organic_signs']))\n",
    "    else:\n",
    "        organic_vars_and_signs = {}\n",
    "    if 'context_vars' in json_model['InputCollect']:\n",
    "        context_vars_and_signs = dict(zip(json_model['InputCollect']['context_vars'], json_model['InputCollect']['context_signs']))\n",
    "    else:\n",
    "        context_vars_and_signs = {}\n",
    "    if 'prophet_vars' in json_model['InputCollect']:\n",
    "        prophet_vars_and_signs = dict(zip(json_model['InputCollect']['prophet_vars'], json_model['InputCollect']['prophet_signs']))\n",
    "    else:\n",
    "        prophet_vars_and_signs = {}\n",
    "    \n",
    "    vars_and_signs = paid_media_vars_and_signs | organic_vars_and_signs | context_vars_and_signs | prophet_vars_and_signs | {'intercept': 'positive'}\n",
    "\n",
    "    # Define a list to store the best models from each trial\n",
    "    models = []\n",
    "\n",
    "    dict_pareto_aggregated_media = {\n",
    "        'cluster': [],\n",
    "        'rn':[],\n",
    "        'solID':[],\n",
    "        'total_spend':[],\n",
    "        'total_response':[],\n",
    "        'cpa_total':[],\n",
    "        'roi_total':[]\n",
    "    }\n",
    "    count_trials = 1\n",
    "\n",
    "    # Run the optimization process for each trial\n",
    "    for trial in range(num_trials):\n",
    "        # Set a random seed for reproducibility\n",
    "        np.random.seed(trial)\n",
    "\n",
    "        parameters=create_parametrization(init_summary_dict, vars_and_signs, type_of_use='instrumentation')       \n",
    "        hyper_updated=create_parametrization(init_summary_dict, vars_and_signs, type_of_use='hyper_updated')\n",
    "\n",
    "        instrum = ng.p.Instrumentation(**parameters)\n",
    "        optimizer= ng.optimizers.TwoPointsDE(instrum, budget=iterations)\n",
    "\n",
    "        optimizer.minimize(robyn_model_obj)\n",
    "\n",
    "        # Store all models from this trial\n",
    "        if optimizer.pareto_front():\n",
    "            trial_models = [col.value[1] for col in optimizer.pareto_front()]\n",
    "        else:\n",
    "            trial_models = [optimizer.recommend().value[1]]\n",
    "        \n",
    "        count_models = 1\n",
    "\n",
    "        for trial_model in trial_models:\n",
    "            new_summary_dict = {}\n",
    "\n",
    "            for key, value in init_summary_dict.items():\n",
    "                if key + '_alphas' in parameters.keys():\n",
    "                    new_summary_dict[key] = {'coef': trial_model[key], 'alphas': trial_model[key + '_alphas'], 'gammas': trial_model[key + '_gammas'], 'shapes': trial_model[key + '_shapes'], 'scales': trial_model[key + '_scales']}\n",
    "                else:\n",
    "                    new_summary_dict[key] = {'coef': trial_model[key]}\n",
    "                \n",
    "            json_model = create_summary_and_hyper_values(json_model, new_summary_dict, init_lambda_value)\n",
    "            \n",
    "            result, model, summary_dict_obj, df_saturation_ridge, lambda_value, ridge_intercept, rsq, nrmse, rssd, mape = import_model(json_model, df, df_holidays, type_of_use='refresh')\n",
    "            \n",
    "            summary_dict_obj[\"rsq\"] = rsq\n",
    "            summary_dict_obj[\"nrmse\"] = nrmse\n",
    "            summary_dict_obj[\"decomp_rssd\"] = rssd\n",
    "            summary_dict_obj[\"mape\"] = mape\n",
    "\n",
    "            df_saturation_ridge_model_window = df_saturation_ridge.loc[(df_saturation_ridge[date_var] >= window_start) & (df_saturation_ridge[date_var] <= window_end)]\n",
    "            \n",
    "            # Add the models from this trial to the list\n",
    "            models.append(summary_dict_obj)\n",
    "\n",
    "            for media in json_model['InputCollect']['paid_media_spends']:\n",
    "                total_spend_model = df_window[media].sum()\n",
    "                total_response_model = sum([value * summary_dict_obj[media]['coef'] for value in df_saturation_ridge_model_window[media]])\n",
    "                dict_pareto_aggregated_media['cluster'].append(1)\n",
    "                dict_pareto_aggregated_media['rn'].append(media)\n",
    "                dict_pareto_aggregated_media['solID'].append(str(count_trials) + '_' + str(count_models))\n",
    "                dict_pareto_aggregated_media['total_spend'].append(total_spend_model)\n",
    "                dict_pareto_aggregated_media['total_response'].append(sum([value * summary_dict_obj[media]['coef'] for value in df_saturation_ridge[media]]))\n",
    "                dict_pareto_aggregated_media['cpa_total'].append(total_spend_model / total_response_model if int(total_response_model) != 0 else 0)\n",
    "                dict_pareto_aggregated_media['roi_total'].append(total_response_model / total_spend_model if int(total_spend_model) != 0 else 0)\n",
    "\n",
    "            count_models += 1\n",
    "        count_trials += 1\n",
    "\n",
    "    if json_model['InputCollect']['dep_var_type'][0] == 'conversion':\n",
    "        dep_var_type = 'C'\n",
    "    else:\n",
    "        dep_var_type = 'R'\n",
    "    \n",
    "    df_pareto_aggregated = pd.DataFrame(dict_pareto_aggregated_media)\n",
    "    share_spend = {}\n",
    "    total_spend = df_window[json_model['InputCollect']['paid_media_spends']].sum().sum()\n",
    "\n",
    "    for media in json_model['InputCollect']['paid_media_spends']:\n",
    "        share_spend[media] = df_window[media].sum() / total_spend\n",
    "        \n",
    "    ci_list = confidence_calcs(df_pareto_aggregated, share_spend, json_model['InputCollect']['paid_media_spends'], dep_var_type, 1)\n",
    "\n",
    "    bootstrap_df = ci_list['df_ci'].reset_index(drop=True)\n",
    "\n",
    "    # Calculate the minimum and maximum values for each error metric\n",
    "    nrmse_values = [model['nrmse'] for model in models]\n",
    "    decomp_rssd_values = [model['decomp_rssd'] for model in models]\n",
    "    mape_values = [model['mape'] for model in models]\n",
    "\n",
    "    nrmse_min, nrmse_max = min(nrmse_values), max(nrmse_values)\n",
    "    decomp_rssd_min, decomp_rssd_max = min(decomp_rssd_values), max(decomp_rssd_values)\n",
    "    \n",
    "    # Check if there's MAPE - Which means calibration used\n",
    "    if mape_values[0] is not None:\n",
    "        mape_min, mape_max = min(mape_values), max(mape_values)\n",
    "    else:\n",
    "        mape_min, mape_max = None, None\n",
    "\n",
    "    # Calculate the error score for each model\n",
    "    for model in models:\n",
    "        model['error_score'] = compute_error_score(model['nrmse'], model['decomp_rssd'], model['mape'], [1, 1, 1], \n",
    "                                                nrmse_min, nrmse_max, decomp_rssd_min, decomp_rssd_max, mape_min, mape_max)\n",
    "\n",
    "    # Select the model with the smallest error score\n",
    "    best_model = min(models, key=lambda model: model['error_score'])\n",
    "\n",
    "    json_model = create_summary_and_hyper_values(json_model, best_model, init_lambda_value)\n",
    "    result, model, summary_dict_obj, df_saturation_ridge, lambda_value, ridge_intercept, rsq, nrmse, rssd, mape = import_model(json_model, df, df_holidays, type_of_use='refresh')\n",
    "    \n",
    "    summary_list = json_model['ExportedModel'][\"summary\"]\n",
    "    \n",
    "    df_saturation_ridge_window = df_saturation_ridge.loc[(df_saturation_ridge[date_var] >= window_start) & (df_saturation_ridge[date_var] <= window_end)]\n",
    "    \n",
    "    for model_summary in summary_list:\n",
    "        if model_summary[\"variable\"] in json_model['InputCollect']['paid_media_spends']:\n",
    "            total_spend_top = df_window[model_summary[\"variable\"]].sum()\n",
    "            total_response_top = sum([value * summary_dict_obj[model_summary[\"variable\"]]['coef'] for value in df_saturation_ridge_window[model_summary[\"variable\"]]])\n",
    "    \n",
    "            if json_model['InputCollect']['dep_var_type'][0] == 'conversion':\n",
    "                cpa_total_top = total_spend_top / total_response_top if int(total_response_top) != 0 else 0\n",
    "            else:\n",
    "                cpa_total_top = total_response_top / total_spend_top if int(total_spend_top) != 0 else 0\n",
    "        \n",
    "            model_summary[\"boot_mean_cassandra\"] = bootstrap_df[bootstrap_df['rn'] == model_summary[\"variable\"]]['boot_mean'].iloc[0]\n",
    "            model_summary[\"ci_low_cassandra\"] = bootstrap_df[bootstrap_df['rn'] == model_summary[\"variable\"]]['ci_low'].iloc[0] if bootstrap_df[bootstrap_df['rn'] == model_summary[\"variable\"]]['ci_low'].iloc[0] < cpa_total_top else cpa_total_top \n",
    "            model_summary[\"ci_up_cassandra\"] = bootstrap_df[bootstrap_df['rn'] == model_summary[\"variable\"]]['ci_up'].iloc[0] if bootstrap_df[bootstrap_df['rn'] == model_summary[\"variable\"]]['ci_up'].iloc[0] > cpa_total_top else cpa_total_top\n",
    "    \n",
    "    return result, model, summary_dict_obj, summary_list, df_saturation_ridge, best_model, hyper_updated, lambda_value, ridge_intercept, rsq, nrmse, rssd, mape\n",
    "\n",
    "# Remember to adjust the balance according to your needs.\n",
    "# For example, if you want to give equal importance to all metrics, you can set balance = [1, 1, 1].\n",
    "# If you want to give more importance to NRMSE and less to the others, you can set balance = [2, 1, 1], and so on.\n",
    "# These weights should be normalized to sum to 1, but the compute_error_score function takes care of that for you.\n",
    "def compute_error_score(nrmse, decomp_rssd, mape=None, balance=None, \n",
    "                    nrmse_min=None, nrmse_max=None, decomp_rssd_min=None, decomp_rssd_max=None, mape_min=None, mape_max=None):\n",
    "    # Normalize the error metrics\n",
    "    nrmse_n = (nrmse - nrmse_min) / (nrmse_max - nrmse_min)\n",
    "    decomp_rssd_n = (decomp_rssd - decomp_rssd_min) / (decomp_rssd_max - decomp_rssd_min)\n",
    "    \n",
    "    # Calculate the weighted sum of the squared error metrics\n",
    "    if mape is not None and mape_min is not None and mape_max is not None:\n",
    "        mape_n = (mape - mape_min) / (mape_max - mape_min)\n",
    "        error_score = math.sqrt(balance[0]*nrmse_n**2 + balance[1]*decomp_rssd_n**2 + balance[2]*mape_n**2)\n",
    "    else:\n",
    "        error_score = math.sqrt(balance[0]*nrmse_n**2 + balance[1]*decomp_rssd_n**2)\n",
    "\n",
    "    return error_score\n",
    "\n",
    "def create_summary_and_hyper_values(json_model, summary_dict, lambda_value):\n",
    "    hyper_values = {}\n",
    "    summary = []\n",
    "\n",
    "    for key, value in summary_dict.items():\n",
    "        if isinstance(value, dict) and 'alphas' in list(value.keys()):\n",
    "            summary.append({'variable': key, 'coef': value['coef']})\n",
    "            hyper_values[key + '_alphas'] = [value['alphas']]\n",
    "            hyper_values[key + '_gammas'] = [value['gammas']]\n",
    "            hyper_values[key + '_scales'] = [value['scales']]\n",
    "            hyper_values[key + '_shapes'] = [value['shapes']]\n",
    "        else:\n",
    "            if key == 'intercept':\n",
    "                summary.append({'variable': '(Intercept)', 'coef': value['coef']})\n",
    "            elif key not in (\"rsq\", \"nrmse\", \"decomp_rssd\", \"mape\", \"error_score\"):\n",
    "                summary.append({'variable': key, 'coef': value['coef']})\n",
    "    \n",
    "    hyper_values['lambda'] = [lambda_value]\n",
    "\n",
    "    json_model['ExportedModel']['summary']=summary\n",
    "    json_model['ExportedModel']['hyper_values']=hyper_values\n",
    "\n",
    "    return json_model\n",
    "\n",
    "def calculate_adjustment_factor(boot_mean, ci_low, ci_up):\n",
    "    factor_low = abs(boot_mean - ci_low) / boot_mean if boot_mean != 0 else 0.3\n",
    "    factor_up = abs(boot_mean - ci_up) / boot_mean if boot_mean != 0 else 0.3\n",
    "\n",
    "    return factor_low, factor_up\n",
    "\n",
    "def create_summary_dictionary(all_vars, hyper_dict, coef_dict):\n",
    "    \"\"\"\n",
    "    Constructs a summary dictionary containing coefficients and hyperparameters for each variable.\n",
    "\n",
    "    Args:\n",
    "        all_vars (list): A list of all variable names.\n",
    "        hyper_dict (dict): A dictionary containing hyperparameters for variables.\n",
    "        coef_dict (dict): A dictionary containing coefficients for variables.\n",
    "\n",
    "    Returns:\n",
    "        dict: A summary dictionary containing structured information about each variable,\n",
    "              including coefficients and hyperparameters.\n",
    "    \"\"\"\n",
    "    summary_dict = {}\n",
    "\n",
    "    for variable in all_vars:\n",
    "        variable_details = {'coef': coef_dict.get(f'{variable}_coef', 0)}  # Default coef to 0 if not found\n",
    "\n",
    "        # Check and include hyperparameters if they exist for the variable\n",
    "        if f'{variable}_alphas' in hyper_dict:\n",
    "            variable_details.update({\n",
    "                'alphas': hyper_dict[f'{variable}_alphas'],\n",
    "                'gammas': hyper_dict[f'{variable}_gammas'],\n",
    "                'shapes': hyper_dict[f'{variable}_shapes'],\n",
    "                'scales': hyper_dict[f'{variable}_scales']\n",
    "            })\n",
    "\n",
    "        # Check and include bootstrapping metrics if they exist for the variable\n",
    "        boot_mean_key = f'{variable}_boot_mean_cassandra'\n",
    "        if boot_mean_key in coef_dict:\n",
    "            variable_details.update({\n",
    "                'boot_mean_cassandra': coef_dict[boot_mean_key],\n",
    "                'ci_up_cassandra': coef_dict[f'{variable}_ci_up_cassandra'],\n",
    "                'ci_low_cassandra': coef_dict[f'{variable}_ci_low_cassandra']\n",
    "            })\n",
    "\n",
    "        summary_dict[variable] = variable_details\n",
    "\n",
    "    # Handle intercept separately\n",
    "    intercept_key = '(Intercept)'\n",
    "    if intercept_key in coef_dict:\n",
    "        summary_dict['intercept'] = {'coef': coef_dict[intercept_key]}\n",
    "    else:\n",
    "        summary_dict['intercept'] = {'coef': 0}\n",
    "\n",
    "    return summary_dict\n",
    "\n",
    "#Pass a dict {channel: {coef:value, alphas:value, gammas:value, ecc...}} and use this for create a Scalar value to pass a Instrumentation\n",
    "def create_parametrization(summary_dict, vars_and_signs, ci_dict={}, type_of_use='instrumentation'):\n",
    "    parameters = {}\n",
    "\n",
    "    for key, value in summary_dict.items():\n",
    "        if 'boot_mean_cassandra' in value: \n",
    "            hyper_variance_low, hyper_variance_up = calculate_adjustment_factor(summary_dict[key]['boot_mean_cassandra'], summary_dict[key]['ci_low_cassandra'], summary_dict[key]['ci_up_cassandra'])\n",
    "            coef_variance_low, coef_variance_up = hyper_variance_low, hyper_variance_up\n",
    "        else:\n",
    "            hyper_variance_low, hyper_variance_up = 0.3, 0.3\n",
    "            coef_variance_low, coef_variance_up = 0.4, 0.4\n",
    "                \n",
    "        if 'alphas' in value:\n",
    "            alpha_low = summary_dict[key]['alphas'] - (summary_dict[key]['alphas'] * hyper_variance_low)\n",
    "            if alpha_low <= 0.3:\n",
    "                alpha_low = 0.3\n",
    "            \n",
    "            alpha_up = summary_dict[key]['alphas'] + (summary_dict[key]['alphas'] * hyper_variance_up)\n",
    "\n",
    "            if alpha_up > 2.5 and alpha_low < 2.5:\n",
    "                alpha_up = 2.5\n",
    "            elif alpha_up <= alpha_low:\n",
    "                alpha_up = alpha_low + 0.1\n",
    "\n",
    "            if type_of_use == 'hyper_updated':\n",
    "                parameters[f'{key}_alphas']=[alpha_low, alpha_up]\n",
    "\n",
    "            elif type_of_use == 'instrumentation':\n",
    "                parameters[f'{key}_alphas']=ng.p.Scalar(lower=alpha_low, upper=alpha_up)\n",
    "                #print(\"TODO: Nevergrad\")\n",
    "        \n",
    "        if 'gammas' in value:\n",
    "            gamma_low = summary_dict[key]['gammas'] - (summary_dict[key]['gammas'] * hyper_variance_low)\n",
    "            if gamma_low <= 0.1:\n",
    "                gamma_low = 0.1\n",
    "            \n",
    "            gamma_up = summary_dict[key]['gammas'] + (summary_dict[key]['gammas'] * hyper_variance_up)\n",
    "            if gamma_up > 0.9 and gamma_low < 0.9:\n",
    "                gamma_up = 0.9\n",
    "            elif gamma_up <= gamma_low:\n",
    "                gamma_up = gamma_low + 0.05\n",
    "\n",
    "            if gamma_up > 1:\n",
    "                gamma_up = 1\n",
    "                gamma_low = gamma_up - 0.05\n",
    "\n",
    "            if type_of_use == 'hyper_updated':\n",
    "                parameters[f'{key}_gammas']=[gamma_low, gamma_up]\n",
    "\n",
    "            elif type_of_use == 'instrumentation':\n",
    "                parameters[f'{key}_gammas']=ng.p.Scalar(lower=gamma_low, upper=gamma_up)\n",
    "                #print(\"TODO: Nevergrad\")\n",
    "        \n",
    "        if 'shapes' in value:\n",
    "            #shape_low = summary_dict[key]['shapes']*0.8\n",
    "            shape_low = summary_dict[key]['shapes'] - (summary_dict[key]['shapes'] * hyper_variance_low)\n",
    "            if shape_low <= 0:\n",
    "                shape_low = 0.1\n",
    "            \n",
    "            #shape_up = summary_dict[key]['shapes']*1.2\n",
    "            shape_up = summary_dict[key]['shapes'] + (summary_dict[key]['shapes'] * hyper_variance_up)\n",
    "            if shape_up > 7:\n",
    "                shape_up = 7\n",
    "\n",
    "            if shape_up <= shape_low:\n",
    "                shape_up = shape_low + 0.5\n",
    "                \n",
    "            if type_of_use == 'hyper_updated':\n",
    "                parameters[f'{key}_shapes']=[shape_low, shape_up]\n",
    "\n",
    "            elif type_of_use == 'instrumentation':\n",
    "                parameters[f'{key}_shapes']=ng.p.Scalar(lower=shape_low, upper=shape_up)\n",
    "                #print(\"TODO: Nevergrad\")\n",
    "        \n",
    "        if 'scales' in value:\n",
    "            #scale_low = summary_dict[key]['scales']*0.8\n",
    "            scale_low = summary_dict[key]['scales'] - (summary_dict[key]['scales'] * hyper_variance_low)\n",
    "            if scale_low <= 0:\n",
    "                scale_low = 0.001\n",
    "            \n",
    "            #scale_up = summary_dict[key]['scales']*1.2\n",
    "            scale_up = summary_dict[key]['scales'] + (summary_dict[key]['scales'] * hyper_variance_up)\n",
    "            if scale_up > 0.25:\n",
    "                scale_up = 0.25\n",
    "\n",
    "            if scale_up <= scale_low:\n",
    "                scale_up = scale_low + 0.005\n",
    "                \n",
    "            if type_of_use == 'hyper_updated':\n",
    "                parameters[f'{key}_scales']=[scale_low, scale_up]\n",
    "\n",
    "            elif type_of_use == 'instrumentation':\n",
    "                parameters[f'{key}_scales']=ng.p.Scalar(lower=scale_low, upper=scale_up)\n",
    "                #print(\"TODO: Nevergrad\")\n",
    "\n",
    "        if type_of_use == 'instrumentation':\n",
    "            if 'coef' in value:\n",
    "                if summary_dict[key]['coef'] != 0:\n",
    "                    coef_low = summary_dict[key]['coef'] - (summary_dict[key]['coef'] * coef_variance_low)\n",
    "\n",
    "                    if coef_low < 0 and vars_and_signs[key] == 'positive':\n",
    "                        coef_low = 0\n",
    "\n",
    "                    coef_up = summary_dict[key]['coef'] + (summary_dict[key]['coef'] * coef_variance_up)\n",
    "\n",
    "                    if coef_up > 0 and vars_and_signs[key] == 'negative':\n",
    "                        coef_up = 0\n",
    "\n",
    "                    if coef_up == 0:\n",
    "                        coef_up = 0.000001\n",
    "                    \n",
    "                    if coef_up == coef_low:\n",
    "                        coef_up = coef_low + 0.001\n",
    "\n",
    "                    if coef_low < coef_up:\n",
    "                        parameters[f'{key}']=ng.p.Scalar(lower=coef_low, upper=coef_up)\n",
    "                    else:\n",
    "                        parameters[f'{key}']=ng.p.Scalar(lower=coef_up, upper=coef_low)\n",
    "                else:\n",
    "                    parameters[f'{key}']=ng.p.Scalar(lower=0, upper=0.000001)\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "def apply_adstock_transformation(df, all_media, summary_dict, date_var):\n",
    "    \"\"\"\n",
    "    Applies adstock transformation to media columns in the dataframe based on provided shapes and scales.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Original dataframe containing media columns and a date column.\n",
    "        all_media (list): List of column names in `df` representing media variables to be adstock transformed.\n",
    "        summary_dict (dict): Dictionary containing 'shapes' and 'scales' for adstock transformation of each media variable.\n",
    "        date_var (str): The name of the date column in `df`.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A new dataframe with adstock-transformed media variables and the original date column.\n",
    "    \"\"\"\n",
    "    df_adstock = df[all_media].copy()\n",
    "    for col in df_adstock.columns:\n",
    "        shape, scale = summary_dict[col]['shapes'], summary_dict[col]['scales']\n",
    "        df_adstock[col] = adstock(df_adstock[col], shape, scale)['x_decayed']\n",
    "    df_adstock.reset_index(inplace=True, drop=True)\n",
    "    df_adstock[date_var] = list(df[date_var])\n",
    "    \n",
    "    return df_adstock\n",
    "\n",
    "def apply_saturation_transformation(df_adstock, all_media, summary_dict, date_var, window_start, window_end):\n",
    "    \"\"\"\n",
    "    Filters the adstock-transformed DataFrame for a specified date window and applies saturation transformations\n",
    "    to media variables using the Hill function based on alphas and gammas from the summary dictionary. Appends\n",
    "    the original date variable to the resulting DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df_adstock (pd.DataFrame): DataFrame containing adstock-transformed media variables and a date column.\n",
    "        all_media (list): List of column names representing media variables to be saturation transformed.\n",
    "        summary_dict (dict): Dictionary containing 'alphas' and 'gammas' for saturation transformation of each media variable.\n",
    "        date_var (str): The name of the date column in `df_adstock`.\n",
    "        window_start (str): The start of the date window for filtering `df_adstock`.\n",
    "        window_end (str): The end of the date window for filtering `df_adstock`.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A new DataFrame with saturation-transformed media variables and the original date column, filtered\n",
    "                      by the specified date window.\n",
    "    \"\"\"\n",
    "    # Filter adstock DataFrame for the specified date window\n",
    "    df_adstock_filtered = df_adstock.loc[(df_adstock[date_var] >= window_start) & (df_adstock[date_var] <= window_end)]\n",
    "    \n",
    "    # Copy the filtered media columns to a new DataFrame for saturation transformation\n",
    "    df_saturation = df_adstock_filtered[all_media].copy()\n",
    "    \n",
    "    # Apply saturation transformation to each media column\n",
    "    for col in df_saturation.columns:\n",
    "        alphas, gammas = summary_dict[col]['alphas'], summary_dict[col]['gammas']\n",
    "        df_saturation[col] = saturation_hill(df_saturation[col], alphas, gammas)\n",
    "    \n",
    "    # Reset index and append the date variable from the filtered DataFrame\n",
    "    df_saturation.reset_index(inplace=True, drop=True)\n",
    "    df_saturation[date_var] = list(df_adstock_filtered[date_var])\n",
    "    \n",
    "    return df_saturation, df_adstock_filtered\n",
    "\n",
    "def apply_decomp_transformation(df_saturation, all_media, summary_dict, date_var):\n",
    "    \"\"\"\n",
    "    Apply decomposition transformation to the given dataframe.\n",
    "\n",
    "    Args:\n",
    "        df_saturation (pandas.DataFrame): The input dataframe containing saturation data.\n",
    "        all_media (list): List of media columns to apply the transformation.\n",
    "        summary_dict (dict): Dictionary containing coefficient values for each media column.\n",
    "        date_var (str): Name of the date variable column.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: The transformed dataframe with decomposition applied.\n",
    "    \"\"\"\n",
    "    df_alldecomp = df_saturation[all_media].copy()\n",
    "\n",
    "    for col in df_alldecomp.columns:\n",
    "        df_alldecomp[col] = summary_dict[col]['coef'] * df_saturation[col]\n",
    "\n",
    "    df_alldecomp[date_var] = list(df_saturation[date_var])\n",
    "\n",
    "    return df_alldecomp\n",
    "\n",
    "def extract_hyperparameters(hyper_values):\n",
    "    \"\"\"\n",
    "    Extracts hyperparameters from the given JSON model, excluding 'lambda',\n",
    "    which is assigned to a separate variable.\n",
    "\n",
    "    Args:\n",
    "        json_model (dict): The JSON model object containing 'ExportedModel' and 'hyper_values'.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A dictionary of hyperparameters and the lambda value extracted from the model.\n",
    "    \"\"\"\n",
    "    hyper_dict = {}\n",
    "    lambda_value = 0\n",
    "\n",
    "    # Iterate through hyper_values, excluding 'lambda'\n",
    "    for key, value in hyper_values.items():\n",
    "        if key != 'lambda':\n",
    "            hyper_dict[key] = value[0]  # Extract the first value of the list for each hyperparameter\n",
    "        else:\n",
    "            lambda_value = value[0]  # Assign lambda value separately\n",
    "\n",
    "    return hyper_dict, lambda_value\n",
    "\n",
    "def extract_coefficients_and_confidence_intervals(summary, paid_media):\n",
    "    \"\"\"\n",
    "    Extracts coefficients and, for paid media variables, bootstrap means and confidence intervals\n",
    "    from the model's summary.\n",
    "\n",
    "    Args:\n",
    "        json_model (dict): The JSON model object containing 'ExportedModel' and its 'summary'.\n",
    "        paid_media (list): A list of strings representing the names of paid media variables.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing coefficients and, for paid media variables,\n",
    "              bootstrap means and confidence intervals.\n",
    "    \"\"\"\n",
    "    coef_dict = {}\n",
    "\n",
    "    for elem in summary:\n",
    "        # Always extract coefficients\n",
    "        coef_key = f\"{elem['variable']}_coef\"\n",
    "        coef_dict[coef_key] = elem['coef']\n",
    "\n",
    "        # For paid media variables, extract additional statistics if available\n",
    "        if elem[\"variable\"] in paid_media:\n",
    "            boot_mean_key = f\"{elem['variable']}_boot_mean_cassandra\"\n",
    "            ci_up_key = f\"{elem['variable']}_ci_up_cassandra\"\n",
    "            ci_low_key = f\"{elem['variable']}_ci_low_cassandra\"\n",
    "\n",
    "            # Check for Cassandra-specific or general bootstrapping metrics\n",
    "            if 'boot_mean_cassandra' in elem:\n",
    "                coef_dict[boot_mean_key] = elem['boot_mean_cassandra']\n",
    "                coef_dict[ci_up_key] = elem['ci_up_cassandra']\n",
    "                coef_dict[ci_low_key] = elem['ci_low_cassandra']\n",
    "            elif 'boot_mean' in elem:\n",
    "                coef_dict[boot_mean_key] = elem['boot_mean']\n",
    "                coef_dict[ci_up_key] = elem['ci_up']\n",
    "                coef_dict[ci_low_key] = elem['ci_low']\n",
    "\n",
    "    return coef_dict\n",
    "\n",
    "def import_model(json_model, df, df_holidays, prophet_future_dataframe_periods=14,\n",
    "    prophet_seasonality_mode='additive', ridge_size=0.2, ridge_positive=True, ridge_random_state=42, type_of_use='import'):\n",
    "    \n",
    "    date_var = json_model['InputCollect']['date_var'][0]\n",
    "    dep_var = json_model['InputCollect']['dep_var'][0]\n",
    "    dep_var_type = json_model['InputCollect']['dep_var_type'][0]\n",
    "    if 'prophet_vars' in json_model['InputCollect']:\n",
    "        prophet_vars = json_model['InputCollect']['prophet_vars']\n",
    "    else:\n",
    "        prophet_vars = []\n",
    "    if 'prophet_country' in json_model['InputCollect']:\n",
    "        prophet_country = json_model['InputCollect']['prophet_country'][0]\n",
    "    else:\n",
    "        prophet_country = '-'\n",
    "        \n",
    "    day_interval = json_model['InputCollect']['dayInterval'][0]\n",
    "    interval_type = json_model['InputCollect']['intervalType'][0]\n",
    "    window_start = json_model['InputCollect']['window_start'][0]\n",
    "    window_end = json_model['InputCollect']['window_end'][0]\n",
    "    paid_media = json_model['InputCollect']['paid_media_spends']\n",
    "    if 'organic_vars' in json_model['InputCollect']:\n",
    "        organic = json_model['InputCollect']['organic_vars']\n",
    "    else:\n",
    "        organic = []\n",
    "    if 'context_vars' in json_model['InputCollect']:\n",
    "        context = json_model['InputCollect']['context_vars']\n",
    "    else:\n",
    "        context = []\n",
    "    all_media = json_model['InputCollect']['all_media']\n",
    "    all_vars = json_model['InputCollect']['all_ind_vars']\n",
    "    \n",
    "    if interval_type == 'day':\n",
    "        prophet_freq='D'\n",
    "    elif interval_type == 'week':\n",
    "        prophet_freq='W'\n",
    "    elif interval_type == 'month':\n",
    "        prophet_freq='M'\n",
    "\n",
    "    df_window = df.loc[(df[date_var] >= window_start) & (df[date_var] <= window_end)]\n",
    "    df_window.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    ####################################################SECTION PROPHET####################################################\n",
    "    \n",
    "    df_copy = df.copy()\n",
    "\n",
    "    df_prophet = prophet_cassandra(df_copy, df_holidays, date_var, dep_var, prophet_vars, window_start='', window_end='',\n",
    "            national_holidays_abbreviation=prophet_country, future_dataframe_periods=prophet_future_dataframe_periods, freq=prophet_freq, seasonality_mode=prophet_seasonality_mode)\n",
    "    \n",
    "    df_prophet = df_prophet.loc[(df_prophet[date_var] >= window_start) & (df_prophet[date_var] <= window_end)]\n",
    "    df_prophet.reset_index(inplace=True, drop=True)\n",
    "    ####################################################SECTION COEFF AND HYPERPARAMETERS####################################################\n",
    "\n",
    "    #take a coef\n",
    "    coef_dict = extract_coefficients_and_confidence_intervals(json_model['ExportedModel']['summary'], paid_media)\n",
    "                \n",
    "    #take a hyperparameters\n",
    "    hyper_dict, lambda_value = extract_hyperparameters(json_model['ExportedModel']['hyper_values'])\n",
    "\n",
    "    summary_dict = create_summary_dictionary(all_vars, hyper_dict, coef_dict)\n",
    "    \n",
    "    use_intercept = True if summary_dict['intercept']['coef'] != 0 else False\n",
    "        \n",
    "    ####################################################SECTION CREATE ADSTOCK AND SATURATION DATASET####################################################\n",
    "\n",
    "    df_adstock = apply_adstock_transformation(df, all_media, summary_dict, date_var)\n",
    "\n",
    "    df_saturation, df_adstock_filtered = apply_saturation_transformation(df_adstock, all_media, summary_dict, date_var, window_start, window_end)\n",
    "\n",
    "    ####################################################SECTION RIDGE REGRESSION####################################################\n",
    "    \n",
    "    df_saturation_ridge = df_saturation.copy()\n",
    "    df_saturation_ridge.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    for var in all_vars: \n",
    "        if var not in df_saturation_ridge.columns:\n",
    "            df_saturation_ridge[var] = list(df_prophet[var])\n",
    "            \n",
    "    df_saturation_ridge[dep_var] = list(df_window[dep_var])\n",
    "    df_saturation_ridge[date_var] = list(df_window[date_var])\n",
    "    df_saturation_ridge.fillna(0, inplace=True)\n",
    "    \n",
    "    ridge_coefs = [value['coef'] for key, value in summary_dict.items() if key != 'intercept']\n",
    "    if use_intercept:\n",
    "        ridge_intercept = summary_dict['intercept']['coef']\n",
    "    else:\n",
    "        ridge_intercept = 0\n",
    "    \n",
    "    ridge_result, ridge_model = ridge(df_saturation_ridge, all_vars, dep_var, lambda_value=lambda_value, size=ridge_size, positive=ridge_positive, random_state=ridge_random_state, coeffs=ridge_coefs, intercept=ridge_intercept, context_vars=context)\n",
    "    all_vars_use_intercept = all_vars.copy()\n",
    "    \n",
    "    if use_intercept:\n",
    "        ridge_result['intercept'] = ridge_intercept\n",
    "        all_vars_use_intercept.append('intercept')\n",
    "    \n",
    "    df_alldecomp_matrix = apply_decomp_transformation(ridge_result, all_vars_use_intercept, summary_dict, date_var)\n",
    "    \n",
    "    df_alldecomp_matrix[dep_var] = list(ridge_result[dep_var])\n",
    "    df_alldecomp_matrix['prediction'] = list(ridge_result['prediction'])\n",
    "\n",
    "    if type_of_use == 'refresh':\n",
    "        # Filter the length of the data for errors to be calculated on Refresh Window only\n",
    "        rsq=get_rsq_v2(ridge_result[dep_var].tail(json_model['InputCollect']['refresh_steps'][0]), ridge_result['prediction'].tail(json_model['InputCollect']['refresh_steps'][0]))\n",
    "        nrmse=get_nrmse_v2(ridge_result[dep_var].tail(json_model['InputCollect']['refresh_steps'][0]), ridge_result['prediction'].tail(json_model['InputCollect']['refresh_steps'][0]))\n",
    "        rssd=get_rssd_v2(df, df_alldecomp_matrix, paid_media, date_var, dep_var, is_refresh=True)\n",
    "\n",
    "        if 'calibration_input' in json_model['InputCollect'] and json_model['InputCollect']['calibration_input']:\n",
    "            calibration_input = json_model['InputCollect']['calibration_input']\n",
    "            calibration_errors = []\n",
    "\n",
    "            for elem in calibration_input:\n",
    "                liftStartDate = elem['liftStartDate'].replace(\"as.Date(\", \"\").strip()\n",
    "                liftEndDate = elem['liftEndDate'].replace(\"as.Date(\", \"\").strip()\n",
    "                lift_response = float(elem['liftAbs'])\n",
    "\n",
    "                df_alldecomp_channel = df_alldecomp_matrix.loc[(df_alldecomp_matrix[date_var] >= liftStartDate) & (df_alldecomp_matrix[date_var] <= liftEndDate)][elem['channel']]\n",
    "                response_channel = df_alldecomp_channel.sum()\n",
    "\n",
    "                calibration_error_channel = get_calibration_error(response_channel, lift_response)\n",
    "\n",
    "                calibration_errors.append(calibration_error_channel)\n",
    "\n",
    "            mape = np.nanmean(calibration_errors)\n",
    "        else:\n",
    "            mape=None\n",
    "        \n",
    "        return ridge_result, ridge_model, summary_dict, df_saturation_ridge, lambda_value, ridge_intercept, rsq, nrmse, rssd, mape\n",
    "    \n",
    "    return ridge_model, ridge_result, df_alldecomp_matrix, df_adstock, df_saturation, summary_dict\n",
    "\n",
    "def prophet_cassandra(df, df_holidays, date_var, dep_var, prophet_vars, window_start='', window_end='',\n",
    "        national_holidays_abbreviation='IT', future_dataframe_periods=28, freq='D', seasonality_mode='additive', is_predict_future=False, is_percentage_result=True):\n",
    "\n",
    "    \"\"\"\n",
    "    Applies the Prophet forecasting model to a dataset, incorporating specified seasonality factors and national holidays. \n",
    "    It optionally predicts future values based on the model and returns a dataframe with the forecasted values and \n",
    "    specified seasonalities.\n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): The main dataset containing historical data for the dependent variable and dates.\n",
    "    - df_holidays (DataFrame): A dataset containing holiday dates for the specified national holidays.\n",
    "    - date_var (str): The column name in `df` representing the date.\n",
    "    - dep_var (str): The column name in `df` representing the dependent variable to forecast.\n",
    "    - prophet_vars (list): A list of variables indicating which seasonalities to include in the model ('trend', 'holiday', 'weekday', 'season', 'monthly').\n",
    "    - window_start (str, optional): The start date for the prediction window (inclusive). Defaults to an empty string, indicating no start limit.\n",
    "    - window_end (str, optional): The end date for the prediction window (inclusive). Defaults to an empty string, indicating no end limit.\n",
    "    - national_holidays_abbreviation (str): The country code for which national holidays should be considered. Use '-' to ignore holidays.\n",
    "    - future_dataframe_periods (int): The number of periods to forecast into the future.\n",
    "    - freq (str): The frequency of the data recording (e.g., 'D' for daily).\n",
    "    - seasonality_mode (str): The type of seasonality ('additive' or 'multiplicative').\n",
    "    - is_predict_future (bool): Flag indicating whether to predict future values beyond the historical data.\n",
    "    - is_percentage_result (bool): Flag indicating whether seasonal effects should be returned as percentages (relative change) or absolute values.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: A dataframe containing the original date column and additional columns for each requested seasonality and trend, \n",
    "                 as well as the forecasted values for the dependent variable. If `is_predict_future` is True, the dataframe will \n",
    "                 be limited to the specified prediction window; otherwise, it merges the forecasted values with the original dataset.\n",
    "\n",
    "    Note:\n",
    "    - The function assumes the presence of `Prophet` from the `prophet` package and requires prior installation of this package.\n",
    "    - This function is specifically designed for use within the Cassandra platform but can be adapted for other purposes.\n",
    "    \"\"\"\n",
    "    \n",
    "    if 'trend' in prophet_vars:\n",
    "        trend_seasonality=True\n",
    "    else:\n",
    "        trend_seasonality=False\n",
    "    if 'holiday' in prophet_vars:\n",
    "        holiday_seasonality=True\n",
    "    else:\n",
    "        holiday_seasonality=False\n",
    "    if 'weekday' in prophet_vars:\n",
    "        weekday_seasonality=True\n",
    "    else:\n",
    "        weekday_seasonality=False\n",
    "    if 'season' in prophet_vars:\n",
    "        season_seasonality=True\n",
    "    else:\n",
    "        season_seasonality=False\n",
    "    if 'monthly' in prophet_vars:\n",
    "        monthly_seasonality=True\n",
    "    else:\n",
    "        monthly_seasonality=False\n",
    "    \n",
    "    # Create a DF with the only two columns for Prophet\n",
    "    prophet_df = df[[date_var, dep_var]]\n",
    "\n",
    "    # Rename the columns for Prophet\n",
    "    prophet_df = prophet_df.rename(columns={date_var: 'ds', dep_var: 'y'})\n",
    "\n",
    "    if national_holidays_abbreviation != '-':\n",
    "        # Select the Holidays according to the country that interests me\n",
    "        condition = (df_holidays['country'] == national_holidays_abbreviation)\n",
    "        \n",
    "        holidays = df_holidays.loc[condition, ['ds', 'holiday']]\n",
    "\n",
    "        # Instance and fit Prophet\n",
    "        prophet_m = Prophet(weekly_seasonality=weekday_seasonality, yearly_seasonality=season_seasonality,\n",
    "                        daily_seasonality=False, holidays=holidays, seasonality_mode=seasonality_mode)\n",
    "    else:\n",
    "        # Instance and fit Prophet\n",
    "        prophet_m = Prophet(weekly_seasonality=weekday_seasonality, yearly_seasonality=season_seasonality,\n",
    "                        daily_seasonality=False, seasonality_mode=seasonality_mode)\n",
    "\n",
    "    if monthly_seasonality:\n",
    "        prophet_m.add_seasonality(name='monthly', period=30.5, fourier_order=5)\n",
    "        \n",
    "    prophet_m.fit(prophet_df)\n",
    "    \n",
    "    future = prophet_m.make_future_dataframe(periods=int(future_dataframe_periods), freq=freq)\n",
    "\n",
    "    forecast = prophet_m.predict(future)\n",
    "    \n",
    "    new_forecast = forecast[['ds', 'yhat', 'trend', 'additive_terms', 'multiplicative_terms']].copy()\n",
    "\n",
    "    if 'yearly' in forecast:\n",
    "        if is_percentage_result:\n",
    "            new_forecast['season'] = forecast['yearly'].copy()\n",
    "        else:\n",
    "            new_forecast['season'] = forecast.apply(\n",
    "                lambda row: row['trend'] * (row['yearly']),\n",
    "                axis=1\n",
    "            ) \n",
    "            \n",
    "    if 'monthly' in forecast:\n",
    "        if is_percentage_result:\n",
    "            new_forecast['monthly'] = forecast['monthly'].copy()\n",
    "        else:\n",
    "            new_forecast['monthly'] = forecast.apply(\n",
    "                lambda row: row['trend'] * row['monthly'],\n",
    "                axis=1\n",
    "            ) \n",
    "        \n",
    "    if 'weekly' in forecast:\n",
    "        if is_percentage_result:\n",
    "            new_forecast['weekday'] = forecast['weekly'].copy()\n",
    "        else:\n",
    "            new_forecast['weekday'] = forecast.apply(\n",
    "                lambda row: row['trend'] * row['weekly'],\n",
    "                axis=1\n",
    "            )\n",
    "            \n",
    "    if 'holidays' in forecast:\n",
    "        if is_percentage_result:\n",
    "            new_forecast['holidays'] = forecast['holidays'].copy()\n",
    "        else:\n",
    "            new_forecast['holidays'] = forecast.apply(\n",
    "                lambda row: row['trend'] * row['holidays'],\n",
    "                axis=1\n",
    "            )\n",
    "\n",
    "    sub_prophet_df = new_forecast[['ds']].copy()\n",
    "\n",
    "    if trend_seasonality:\n",
    "        sub_prophet_df['trend'] = new_forecast['trend']\n",
    "    if holiday_seasonality:\n",
    "        sub_prophet_df['holiday'] = new_forecast['holidays']\n",
    "    if 'season' in new_forecast:\n",
    "        sub_prophet_df['season'] = new_forecast['season']\n",
    "    if 'weekday' in new_forecast:\n",
    "        sub_prophet_df['weekday'] = new_forecast['weekday']\n",
    "    if 'monthly' in new_forecast:\n",
    "        sub_prophet_df['monthly'] = new_forecast['monthly']\n",
    "\n",
    "    sub_prophet_df = sub_prophet_df.rename(columns={'ds': date_var})\n",
    "\n",
    "    df[date_var] = pd.to_datetime(df[date_var])\n",
    "    sub_prophet_df[date_var] = pd.to_datetime(sub_prophet_df[date_var]) \n",
    "    \n",
    "    if is_predict_future:\n",
    "        if window_start and window_end:\n",
    "            sub_prophet_df_window = sub_prophet_df.loc[(sub_prophet_df[date_var] >= window_start) & (sub_prophet_df[date_var] <= window_end)]\n",
    "        elif window_start and not window_end:\n",
    "            sub_prophet_df_window = sub_prophet_df.loc[sub_prophet_df[date_var] >= window_start]\n",
    "        elif not window_start and window_end:\n",
    "            sub_prophet_df_window = sub_prophet_df.loc[sub_prophet_df[date_var] <= window_end]\n",
    "        else:\n",
    "            sub_prophet_df_window = sub_prophet_df\n",
    "        \n",
    "        return sub_prophet_df_window\n",
    "\n",
    "    # Step 1: Identify Overlapping Columns\n",
    "    overlapping_columns = set(df.columns).intersection(sub_prophet_df.columns)\n",
    "    overlapping_columns.remove(date_var)  # Exclude the merge column\n",
    "\n",
    "    full_df = pd.merge(df, sub_prophet_df, how='inner', on=date_var)\n",
    "\n",
    "    # Step 3: Keep Columns from sub_prophet_df\n",
    "    for col in overlapping_columns:\n",
    "        full_df[col] = full_df[col + '_y']\n",
    "        full_df.drop([col + '_x', col + '_y'], axis=1, inplace=True)\n",
    "\n",
    "    if window_start and window_end:\n",
    "        df_window = full_df.loc[(full_df[date_var] >= window_start) & (full_df[date_var] <= window_end)]\n",
    "    elif window_start and not window_end:\n",
    "        df_window = full_df.loc[full_df[date_var] >= window_start]\n",
    "    elif not window_start and window_end:\n",
    "        df_window = full_df.loc[full_df[date_var] <= window_end]\n",
    "    else:\n",
    "        df_window = full_df\n",
    "    \n",
    "    return df_window\n",
    "\n",
    "def adstock(x, shape, scale, type=\"pdf\"):\n",
    "    windlen = len(x)\n",
    "    x_decayed = np.zeros(windlen)\n",
    "    thetaVecCum = np.zeros(windlen)\n",
    "    \n",
    "    if windlen <= 1:\n",
    "        x_decayed = np.array(x)\n",
    "        thetaVecCum = np.zeros(windlen)\n",
    "        x_imme = None\n",
    "    \n",
    "    else:\n",
    "        x_bin = np.arange(1, windlen + 1)\n",
    "        scale_trans = np.round(np.quantile(np.arange(1, windlen + 1), scale), 0)\n",
    "        '''if type.lower() == \"cdf\":\n",
    "            thetaVec = np.concatenate(([1], 1 - weibull_min.cdf(x_bin[:-1], shape, scale=scaleTrans)))\n",
    "            thetaVecCum = np.cumprod(thetaVec)'''\n",
    "            \n",
    "        '''if type.lower() == \"pdf\":'''\n",
    "        thetaVecCum = normalize(weibull_min.pdf(x_bin, c=shape, scale=scale_trans))\n",
    "        \n",
    "        for i, x_val in enumerate(x):\n",
    "            x_vec = np.concatenate((np.zeros(i), np.repeat(x_val, windlen - i)))\n",
    "            thetaVecCumLag = np.concatenate((np.zeros(i), thetaVecCum[:windlen-i]))\n",
    "            x_decayed += x_vec * thetaVecCumLag\n",
    "        \n",
    "        x_imme = np.diag(np.outer(x, thetaVecCum))\n",
    "            \n",
    "        '''x_decayed = [decay(x_val, x_pos, theta_vec_cum, windlen) for x_val, x_pos in zip(x, x_bin[:len(x)])]\n",
    "        x_imme = np.diag(x_decayed)\n",
    "        x_decayed = np.sum(x_decayed, axis=0)'''\n",
    "    \n",
    "    inflation_total = np.sum(x_decayed) / np.sum(x) if np.sum(x) != 0 else 0\n",
    "    \n",
    "    return {\"x\": x, \"x_decayed\": x_decayed, \"theta_vec_cum\": thetaVecCum, \"inflation_total\":inflation_total, \"x_imme\": x_imme}\n",
    "\n",
    "def normalize(x):\n",
    "    \"\"\"Normalize the input array.\"\"\"\n",
    "    range_x = np.max(x) - np.min(x)\n",
    "    if range_x == 0:\n",
    "        return np.concatenate(([1], np.zeros(len(x) - 1)))\n",
    "    else:\n",
    "        return (x - np.min(x)) / range_x\n",
    "\n",
    "def decay(x_val, x_pos, theta_vec_cum, windlen):\n",
    "    x_vec = np.concatenate([np.zeros(x_pos - 1), np.full(windlen - x_pos + 1, x_val)])\n",
    "    theta_vec_cum_lag = list(pd.Series(theta_vec_cum.copy()).shift(periods=x_pos-1, fill_value=0))\n",
    "    x_prod = x_vec * theta_vec_cum_lag\n",
    "    return x_prod\n",
    "\n",
    "def saturation_hill(x, alpha, gamma, x_marginal=None):\n",
    "    inflexion = (np.min(x) * (1 - gamma)) + (np.max(x) * gamma)# linear interpolation by dot product\n",
    "    if x_marginal is None:\n",
    "        x_scurve = x**alpha / (x**alpha + inflexion**alpha) # plot(x_scurve) summary(x_scurve)\n",
    "    else:\n",
    "        x_scurve = x_marginal**alpha / (x_marginal**alpha + inflexion**alpha)\n",
    "    return x_scurve\n",
    "\n",
    "def ridge(df, all_vars, dep_var, lambda_value=0, size=0.2, positive=False, random_state=42, coeffs=[], intercept=0, fit_intercept=True, context_vars=[]):\n",
    "        \n",
    "        if context_vars:\n",
    "            has_string = {col: df[col].apply(type).eq(str).any() for col in context_vars}\n",
    "\n",
    "            for col, contains_str in has_string.items():\n",
    "                if contains_str:\n",
    "                    unique_strings = df.loc[df[col].apply(type).eq(str), col].unique()\n",
    "                    mapping_dict = {unique_str: idx + 1 for idx, unique_str in enumerate(unique_strings)}\n",
    "                    df[col] = df[col].replace(mapping_dict)\n",
    "        \n",
    "        X = df[all_vars]\n",
    "        y = df[dep_var]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=size, random_state=random_state)\n",
    "\n",
    "        model = Ridge(alpha=lambda_value, fit_intercept=fit_intercept, positive=positive)\n",
    "        model.intercept_ = intercept\n",
    "        model.coef_ = np.array(coeffs)\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        model.intercept_ = intercept\n",
    "        model.coef_ = np.array(coeffs)\n",
    "\n",
    "        # Ask the model to predict on X_test without having Y_test\n",
    "        # This will give you exact predicted values\n",
    "\n",
    "        # We can use our NRMSE and MAPE functions as well\n",
    "\n",
    "        # Create new DF not to edit the original one\n",
    "        result = df.copy()\n",
    "\n",
    "        # Create a new column with predicted values\n",
    "        result['prediction'] = model.predict(X)\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_test_pred = model.predict(X_test)\n",
    "\n",
    "        metrics_values = {}\n",
    "\n",
    "        return result, model\n",
    "\n",
    "def get_rsq_v2(y_true, y_pred):\n",
    "    if len(y_true) == 1 and len(y_pred) == 1:\n",
    "        difference = abs(y_true[0] - y_pred[0])\n",
    "        value = max(0, min(100, 100 - difference * 10))\n",
    "    else:\n",
    "        corr_matrix = np.corrcoef(list(y_true), list(y_pred))\n",
    "        corr = corr_matrix[0,1]\n",
    "        value = corr**2\n",
    "        value = max(0, min(100, value))\n",
    "    \n",
    "    return value\n",
    "\n",
    "# Se si desidera confrontare errori tra serie con diversi range, questa versione potrebbe essere pi appropriata.\n",
    "def get_nrmse_v2(y_true, y_pred):\n",
    "    if len(y_true) == 1 and len(y_pred) == 1:\n",
    "        if y_true[0] != 0:\n",
    "            difference = abs(y_true[0] - y_pred[0]) / y_true[0]\n",
    "            value = min(100, difference * 100)\n",
    "        else: \n",
    "            value = 0\n",
    "\n",
    "    else:\n",
    "        y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "\n",
    "        rmse = np.sqrt(np.mean((y_true - y_pred)**2))\n",
    "        range_y = np.max(y_true) - np.min(y_true)\n",
    "\n",
    "        value = rmse / range_y\n",
    "\n",
    "        value = max(0, min(100, value))\n",
    "    \n",
    "    return value\n",
    "\n",
    "def get_mape(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    \n",
    "    # Avoid division by zero and calculate MAPE\n",
    "    mask = y_true != 0\n",
    "    mape = (abs((y_true - y_pred) / y_true)[mask]).mean()\n",
    "        \n",
    "    return mape\n",
    "\n",
    "def get_rssd_v2(df, df_alldecomp, media, date_var, dep_var, is_refresh=False):\n",
    "    \n",
    "    if is_refresh:\n",
    "\n",
    "        start_date = min(df_alldecomp[date_var])\n",
    "        end_date = max(df_alldecomp[date_var])\n",
    "\n",
    "        df_media_spend = df.copy()[media]\n",
    "        df_media_spend[date_var] = df[date_var]\n",
    "        df_media_spend = df_media_spend.loc[(df_media_spend[date_var] >= start_date) & (df_media_spend[date_var] <= end_date)]\n",
    "        \n",
    "        share_value = {}\n",
    "        df_spend = df_media_spend.copy()\n",
    "        \n",
    "        df_waterfall = df_alldecomp.copy()\n",
    "        df_waterfall.drop([date_var, dep_var, 'prediction'], axis=1, inplace=True)\n",
    "        \n",
    "        df_waterfall.drop_duplicates(inplace=True)\n",
    "                            \n",
    "        decomp_channels = list(df_waterfall.columns)\n",
    "        if \"trend\" in decomp_channels and \"intercept\" in decomp_channels:\n",
    "            decomp_channels.remove(\"intercept\")\n",
    "        df_spend.drop(date_var, axis=1, inplace=True)\n",
    "\n",
    "    else:\n",
    "\n",
    "        start_date = min(df_alldecomp['ds'])\n",
    "        end_date = max(df_alldecomp['ds'])\n",
    "\n",
    "        df_media_spend = df.copy()[media]\n",
    "        df_media_spend['ds'] = df[date_var]\n",
    "        df_media_spend = df_media_spend.loc[(df_media_spend['ds'] >= start_date) & (df_media_spend['ds'] <= end_date)]\n",
    "\n",
    "        share_value = {}\n",
    "        df_spend = df_media_spend.copy()\n",
    "\n",
    "        df_waterfall = df_alldecomp.copy()\n",
    "        df_waterfall.drop(['ds', 'dep_var', 'depVarHat', 'solID'], axis=1, inplace=True)\n",
    "\n",
    "        columns_to_drop = ['Unnamed: 0', 'refreshStatus', 'bestModRF', 'cluster', 'top_sol']\n",
    "    \n",
    "        for column in columns_to_drop:\n",
    "            if column in df_waterfall.columns:\n",
    "                df_waterfall.drop(column, axis=1, inplace=True)\n",
    "\n",
    "        df_waterfall.drop_duplicates(inplace=True)\n",
    "\n",
    "        decomp_channels = list(df_waterfall.columns)\n",
    "        if \"trend\" in decomp_channels and \"intercept\" in decomp_channels:\n",
    "            decomp_channels.remove(\"intercept\")\n",
    "        df_spend.drop('ds', axis=1, inplace=True)\n",
    "\n",
    "    decomp_decomposition_spend = []\n",
    "    decomp_decomposition_response = []\n",
    "\n",
    "    for channels in list(df_spend.columns):\n",
    "        decomp_decomposition_spend.append(round(sum(df_spend[channels]), 2))\n",
    "    decomp_ch_dec_spend_dirty = dict(zip(list(df_spend.columns), decomp_decomposition_spend))\n",
    "\n",
    "    for channels in decomp_channels:\n",
    "        decomp_decomposition_response.append(round(sum(df_waterfall[channels]), 2))\n",
    "    decomp_ch_dec_response_dirty = dict(zip(decomp_channels, decomp_decomposition_response))\n",
    "\n",
    "    share_value['channels']=[]\n",
    "    share_value['spend'] = []\n",
    "    share_value['effect'] = []\n",
    "\n",
    "    for ch in decomp_channels:\n",
    "        if ch in media:\n",
    "            ch_spend = 0 \n",
    "            ch_response = 0  \n",
    "            for key, value in decomp_ch_dec_response_dirty.items():\n",
    "                if key == ch:\n",
    "                    ch_response = value\n",
    "\n",
    "            for key, value in decomp_ch_dec_spend_dirty.items():\n",
    "                if key == ch:  \n",
    "                    if not isinstance(check_if_exist_value(value), str):\n",
    "                        ch_spend = value\n",
    "                        \n",
    "            share_value['channels'].append(ch)\n",
    "            share_value['spend'].append(ch_spend)\n",
    "            share_value['effect'].append(ch_response)\n",
    "    \n",
    "    all_spend = sum(share_value['spend'])\n",
    "    all_effect = sum(share_value['effect'])\n",
    "\n",
    "    channels_dict = {}\n",
    "    channels_dict['channels']=[]\n",
    "    channels_dict['spend'] = []\n",
    "    channels_dict['effect'] = []\n",
    "\n",
    "    for ch in decomp_channels:\n",
    "        if ch in media:\n",
    "            ch_perc_spend = 0 \n",
    "            ch_perc_response = 0\n",
    "            for key, value in decomp_ch_dec_response_dirty.items():\n",
    "                if key == ch:\n",
    "                    if not isinstance(check_if_exist_value(value), str):\n",
    "                        ch_perc_response = round((value / all_effect), 2)\n",
    "\n",
    "            for key, value in decomp_ch_dec_spend_dirty.items():\n",
    "                if key == ch:  \n",
    "                    if not isinstance(check_if_exist_value(value), str):\n",
    "                        ch_perc_spend = round((value / all_spend), 2)\n",
    "        \n",
    "            channels_dict['channels'].append(ch)\n",
    "            channels_dict['spend'].append(ch_perc_spend)\n",
    "            channels_dict['effect'].append(ch_perc_response)\n",
    "\n",
    "    value = np.sqrt(np.sum((np.array(channels_dict['effect']) - np.array(channels_dict['spend'])) ** 2))\n",
    "\n",
    "    value = max(0, min(100, value))\n",
    "    \n",
    "    return value\n",
    "\n",
    "def get_calibration_error(effect_share, input_calibration):\n",
    "    value = abs(round((effect_share - input_calibration) / input_calibration, 2))\n",
    "\n",
    "    if value > 100:\n",
    "        value = 100\n",
    "    elif value < 0:\n",
    "        value = 0\n",
    "    \n",
    "    return value\n",
    "\n",
    "def create_error_metrics(json_model, df, df_alldecomp, key_export_model = 'ExportedModel'):\n",
    "    \n",
    "    all_sol_id = json_model[key_export_model]['select_model'] + json_model['InputCollect']['refreshSourceID'] if 'refreshSourceID' in json_model['InputCollect'] else json_model[key_export_model]['select_model']\n",
    "    df_alldecomp_new = df_alldecomp.loc[(df_alldecomp['solID'].isin(all_sol_id))]\n",
    "    all_media = json_model['InputCollect']['all_media']\n",
    "    paid_media = json_model['InputCollect']['paid_media_spends']\n",
    "    date_var = json_model['InputCollect']['date_var'][0]\n",
    "    dep_var = json_model['InputCollect']['dep_var'][0]\n",
    "\n",
    "    columns_to_drop = ['Unnamed: 0', 'refreshStatus', 'bestModRF', 'cluster', 'top_sol']\n",
    "    \n",
    "    for column in columns_to_drop:\n",
    "        if column in df_alldecomp_new.columns:\n",
    "            df_alldecomp_new.drop(column, axis=1, inplace=True)\n",
    "        \n",
    "    #df_alldecomp_new = df_alldecomp_new[['ds', 'dep_var', 'depVarHat']]\n",
    "    df_alldecomp_new.drop_duplicates(inplace=True)\n",
    "    is_jsons_model_changed = False\n",
    "    is_train_test_validation = False\n",
    "\n",
    "    if 'train_size' in json_model[key_export_model]['hyper_values'] and json_model[key_export_model]['hyper_values']['train_size'][0] < 1:\n",
    "        is_train_test_validation = True\n",
    "\n",
    "        train_size = json_model[key_export_model]['hyper_values']['train_size'][0]\n",
    "        test_size, validation_size = (1 - train_size) / 2, (1 - train_size) / 2\n",
    "\n",
    "        total_rows = len(df_alldecomp_new)\n",
    "        \n",
    "        train_split = int(total_rows * train_size)\n",
    "        validation_split = train_split + int(total_rows * validation_size)\n",
    "        test_split = validation_split + int(total_rows * test_size)\n",
    "        remaining_rows = total_rows - test_split\n",
    "\n",
    "        if remaining_rows == 1:\n",
    "            test_split += 1 \n",
    "        else:\n",
    "            additional_validation_rows = remaining_rows // 2\n",
    "            additional_test_rows = remaining_rows - additional_validation_rows\n",
    "\n",
    "            validation_split += additional_validation_rows\n",
    "            test_split += additional_test_rows + additional_validation_rows \n",
    "\n",
    "        # Split the DataFrame\n",
    "        train_df = df_alldecomp_new.iloc[:train_split]\n",
    "        validation_df = df_alldecomp_new.iloc[train_split:validation_split]\n",
    "        test_df = df_alldecomp_new.iloc[validation_split:test_split]\n",
    "\n",
    "        if 'rsq_train_cassandra' not in json_model[key_export_model]['errors'][0]:\n",
    "            is_jsons_model_changed = True\n",
    "\n",
    "            rsq_train_cassandra = get_rsq_v2(train_df['dep_var'], train_df['depVarHat'])\n",
    "\n",
    "            if rsq_train_cassandra > 1:\n",
    "                rsq_train_cassandra = 1\n",
    "            elif rsq_train_cassandra < 0:\n",
    "                rsq_train_cassandra = 0\n",
    "\n",
    "            json_model[key_export_model]['errors'][0]['rsq_train_cassandra'] = rsq_train_cassandra\n",
    "\n",
    "        else:\n",
    "            rsq_train_cassandra = json_model[key_export_model]['errors'][0]['rsq_train_cassandra']\n",
    "        \n",
    "        if 'rsq_test_cassandra' not in json_model[key_export_model]['errors'][0]:\n",
    "            is_jsons_model_changed = True\n",
    "\n",
    "            rsq_test_cassandra = get_rsq_v2(test_df['dep_var'], test_df['depVarHat'])\n",
    "\n",
    "            if rsq_test_cassandra > 1:\n",
    "                rsq_test_cassandra = 1\n",
    "            elif rsq_test_cassandra < 0:\n",
    "                rsq_test_cassandra = 0\n",
    "\n",
    "            json_model[key_export_model]['errors'][0]['rsq_test_cassandra'] = rsq_test_cassandra\n",
    "\n",
    "        else:\n",
    "            rsq_test_cassandra = json_model[key_export_model]['errors'][0]['rsq_test_cassandra']\n",
    "        \n",
    "        if 'rsq_validation_cassandra' not in json_model[key_export_model]['errors'][0]:\n",
    "            is_jsons_model_changed = True\n",
    "\n",
    "            rsq_validation_cassandra = get_rsq_v2(validation_df['dep_var'], validation_df['depVarHat'])\n",
    "\n",
    "            if rsq_validation_cassandra > 1:\n",
    "                rsq_validation_cassandra = 1\n",
    "            elif rsq_validation_cassandra < 0:\n",
    "                rsq_validation_cassandra = 0\n",
    "\n",
    "            json_model[key_export_model]['errors'][0]['rsq_validation_cassandra'] = rsq_validation_cassandra\n",
    "\n",
    "        else:\n",
    "            rsq_validation_cassandra = json_model[key_export_model]['errors'][0]['rsq_validation_cassandra']\n",
    "        \n",
    "        if 'nrmse_train_cassandra' not in json_model[key_export_model]['errors'][0]:\n",
    "            is_jsons_model_changed = True\n",
    "            nrmse_train_cassandra = get_nrmse_v2(train_df['dep_var'], train_df['depVarHat'])\n",
    "\n",
    "            if nrmse_train_cassandra > 1:\n",
    "                nrmse_train_cassandra = 1\n",
    "            elif nrmse_train_cassandra < 0:\n",
    "                nrmse_train_cassandra = 0\n",
    "\n",
    "            json_model[key_export_model]['errors'][0]['nrmse_train_cassandra'] = nrmse_train_cassandra\n",
    "\n",
    "        else:\n",
    "            nrmse_train_cassandra = json_model[key_export_model]['errors'][0]['nrmse_train_cassandra']\n",
    "        \n",
    "        if 'nrmse_test_cassandra' not in json_model[key_export_model]['errors'][0]:\n",
    "            is_jsons_model_changed = True\n",
    "            nrmse_test_cassandra = get_nrmse_v2(test_df['dep_var'], test_df['depVarHat'])\n",
    "\n",
    "            if nrmse_test_cassandra > 1:\n",
    "                nrmse_test_cassandra = 1\n",
    "            elif nrmse_test_cassandra < 0:\n",
    "                nrmse_test_cassandra = 0\n",
    "\n",
    "            json_model[key_export_model]['errors'][0]['nrmse_test_cassandra'] = nrmse_test_cassandra\n",
    "\n",
    "        else:\n",
    "            nrmse_test_cassandra = json_model[key_export_model]['errors'][0]['nrmse_test_cassandra']\n",
    "        \n",
    "        if 'nrmse_validation_cassandra' not in json_model[key_export_model]['errors'][0]:\n",
    "            is_jsons_model_changed = True\n",
    "\n",
    "            nrmse_validation_cassandra = get_nrmse_v2(validation_df['dep_var'], validation_df['depVarHat'])\n",
    "\n",
    "            if nrmse_validation_cassandra > 1:\n",
    "                nrmse_validation_cassandra = 1\n",
    "            elif nrmse_validation_cassandra < 0:\n",
    "                nrmse_validation_cassandra = 0\n",
    "\n",
    "            json_model[key_export_model]['errors'][0]['nrmse_validation_cassandra'] = nrmse_validation_cassandra\n",
    "\n",
    "        else:\n",
    "            nrmse_validation_cassandra = json_model[key_export_model]['errors'][0]['nrmse_validation_cassandra']\n",
    "        \n",
    "    if 'rsq_cassandra' not in json_model[key_export_model]['errors'][0]:\n",
    "        is_jsons_model_changed = True     \n",
    "\n",
    "        rsq_cassandra = get_rsq_v2(df_alldecomp_new['dep_var'], df_alldecomp_new['depVarHat'])\n",
    "\n",
    "        if rsq_cassandra > 1:\n",
    "            rsq_cassandra = 1\n",
    "        elif rsq_cassandra < 0:\n",
    "            rsq_cassandra = 0\n",
    "\n",
    "        json_model[key_export_model]['errors'][0]['rsq_cassandra'] = rsq_cassandra\n",
    "\n",
    "    else:\n",
    "        rsq_cassandra = json_model[key_export_model]['errors'][0]['rsq_cassandra']\n",
    "    \n",
    "    if 'nrmse_cassandra' not in json_model[key_export_model]['errors'][0]:\n",
    "        is_jsons_model_changed = True\n",
    "        nrmse_cassandra = get_nrmse_v2(df_alldecomp_new['dep_var'], df_alldecomp_new['depVarHat'])\n",
    "\n",
    "        if nrmse_cassandra > 1:\n",
    "            nrmse_cassandra = 1\n",
    "        elif nrmse_cassandra < 0:\n",
    "            nrmse_cassandra = 0\n",
    "\n",
    "        json_model[key_export_model]['errors'][0]['nrmse_cassandra'] = nrmse_cassandra\n",
    "\n",
    "    else:\n",
    "        nrmse_cassandra = json_model[key_export_model]['errors'][0]['nrmse_cassandra']\n",
    "        \n",
    "    if 'mape_cassandra' not in json_model[key_export_model]['errors'][0]:\n",
    "        is_jsons_model_changed = True\n",
    "        mape_cassandra = get_mape(df_alldecomp_new['dep_var'], df_alldecomp_new['depVarHat'])\n",
    "\n",
    "        if mape_cassandra > 1:\n",
    "            mape_cassandra = 1\n",
    "        elif mape_cassandra < 0:\n",
    "            mape_cassandra = 0\n",
    "\n",
    "        json_model[key_export_model]['errors'][0]['mape_cassandra'] = mape_cassandra\n",
    "\n",
    "    else:\n",
    "        mape_cassandra = json_model[key_export_model]['errors'][0]['mape_cassandra']\n",
    "    \n",
    "    if 'rssd_cassandra' not in json_model[key_export_model]['errors'][0]:\n",
    "        is_jsons_model_changed = True\n",
    "        coefs = []\n",
    "        for media in all_media:\n",
    "            for summary in json_model[key_export_model]['summary']:\n",
    "                if summary['variable'] == media:\n",
    "                    coefs.append(summary['coef'])\n",
    "                    break\n",
    "\n",
    "        rssd_cassandra = get_rssd_v2(df, df_alldecomp_new, paid_media, date_var, dep_var)\n",
    "        if rssd_cassandra > 1:\n",
    "            rssd_cassandra = 1\n",
    "        elif rssd_cassandra < 0:\n",
    "            rssd_cassandra = 0\n",
    "\n",
    "        json_model[key_export_model]['errors'][0]['rssd_cassandra'] = rssd_cassandra\n",
    "    else:\n",
    "        rssd_cassandra = json_model[key_export_model]['errors'][0]['rssd_cassandra']\n",
    "\n",
    "    return json_model, is_jsons_model_changed, is_train_test_validation\n",
    "\n",
    "def check_if_exist_value(elem):\n",
    "    if pd.isna(elem) or math.isinf(elem) or elem == 0:\n",
    "        elem = ' - '\n",
    "    \n",
    "    return elem\n",
    "\n",
    "def create_confidence_interval(json_model, cluster_dict, bootstrap_df, share_spend, dep_var_type='O', key_export_model = 'ExportedModel'):\n",
    "    \n",
    "    paid_media = json_model['InputCollect']['paid_media_spends']\n",
    "    target_solID = json_model[key_export_model][\"select_model\"][0]\n",
    "    is_jsons_model_changed = False\n",
    "    exist_ci_low_cassandra = True\n",
    "\n",
    "    # Trovare il cluster corrispondente al solID\n",
    "    target_cluster = None\n",
    "    for cluster, solIDs in cluster_dict.items():\n",
    "        if target_solID in solIDs:\n",
    "            target_cluster = cluster\n",
    "            break\n",
    "\n",
    "    for elem in json_model[key_export_model]['summary']:\n",
    "        if elem['variable'] == paid_media[0]:\n",
    "            if 'ci_low_cassandra' not in elem.keys():\n",
    "                exist_ci_low_cassandra = False\n",
    "                break\n",
    "\n",
    "    if not exist_ci_low_cassandra:\n",
    "        is_jsons_model_changed = True\n",
    "        for model_summary in json_model[key_export_model][\"summary\"]:\n",
    "            if model_summary[\"variable\"] in paid_media:\n",
    "                if not bootstrap_df[(bootstrap_df['cluster'] == target_cluster) & (bootstrap_df['rn'] == model_summary[\"variable\"])]['boot_mean'].empty:\n",
    "                    model_summary[\"boot_mean_cassandra\"] = bootstrap_df[(bootstrap_df['cluster'] == target_cluster) & (bootstrap_df['rn'] == model_summary[\"variable\"])]['boot_mean'].iloc[0]\n",
    "                else:\n",
    "                    model_summary[\"boot_mean_cassandra\"] = 0\n",
    "                \n",
    "                if not bootstrap_df[(bootstrap_df['cluster'] == target_cluster) & (bootstrap_df['rn'] == model_summary[\"variable\"])]['ci_up'].empty and not bootstrap_df[(bootstrap_df['cluster'] == target_cluster) & (bootstrap_df['rn'] == model_summary[\"variable\"])]['ci_low'].empty:\n",
    "                    ci_up_cassandra = bootstrap_df[(bootstrap_df['cluster'] == target_cluster) & (bootstrap_df['rn'] == model_summary[\"variable\"])]['ci_up'].iloc[0]\n",
    "                    ci_low_cassandra = bootstrap_df[(bootstrap_df['cluster'] == target_cluster) & (bootstrap_df['rn'] == model_summary[\"variable\"])]['ci_low'].iloc[0]\n",
    "                        \n",
    "                    ci_percentage = 1 - share_spend[model_summary[\"variable\"]]\n",
    "                    ci_width = model_summary[\"boot_mean_cassandra\"] * ci_percentage\n",
    "\n",
    "                    if ci_up_cassandra == ci_low_cassandra:\n",
    "                        model_summary[\"ci_up_cassandra\"] = ci_up_cassandra + ci_width\n",
    "                        model_summary[\"ci_low_cassandra\"] = ci_low_cassandra - ci_width\n",
    "                    else:\n",
    "                        model_summary[\"ci_up_cassandra\"] = ci_up_cassandra if ci_up_cassandra != model_summary[\"boot_mean_cassandra\"] else model_summary[\"boot_mean_cassandra\"] + abs(ci_low_cassandra - model_summary[\"boot_mean_cassandra\"])\n",
    "                        model_summary[\"ci_low_cassandra\"] = ci_low_cassandra if ci_low_cassandra != model_summary[\"boot_mean_cassandra\"] else model_summary[\"boot_mean_cassandra\"] - abs(ci_up_cassandra - model_summary[\"boot_mean_cassandra\"])\n",
    "\n",
    "                else:\n",
    "                \n",
    "                    if not bootstrap_df[(bootstrap_df['cluster'] == target_cluster) & (bootstrap_df['rn'] == model_summary[\"variable\"])]['ci_up'].empty:\n",
    "                        ci_up_cassandra = bootstrap_df[(bootstrap_df['cluster'] == target_cluster) & (bootstrap_df['rn'] == model_summary[\"variable\"])]['ci_up'].iloc[0]\n",
    "                        ci_percentage = 1 - share_spend[model_summary[\"variable\"]]\n",
    "                        ci_width = model_summary[\"boot_mean_cassandra\"] * ci_percentage\n",
    "                        \n",
    "                        model_summary[\"ci_up_cassandra\"] = ci_up_cassandra if ci_up_cassandra != model_summary[\"boot_mean_cassandra\"] else ci_up_cassandra + ci_width\n",
    "                    else:\n",
    "                        model_summary[\"ci_up_cassandra\"] = 0\n",
    "                    \n",
    "                    if not bootstrap_df[(bootstrap_df['cluster'] == target_cluster) & (bootstrap_df['rn'] == model_summary[\"variable\"])]['ci_low'].empty:\n",
    "                        ci_low_cassandra = bootstrap_df[(bootstrap_df['cluster'] == target_cluster) & (bootstrap_df['rn'] == model_summary[\"variable\"])]['ci_low'].iloc[0]\n",
    "                        ci_percentage = 1 - share_spend[model_summary[\"variable\"]]\n",
    "                        ci_width = model_summary[\"boot_mean_cassandra\"] * ci_percentage\n",
    "\n",
    "                        model_summary[\"ci_low_cassandra\"] = ci_low_cassandra if ci_low_cassandra != model_summary[\"boot_mean_cassandra\"] else ci_low_cassandra - ci_width\n",
    "                    else:\n",
    "                        model_summary[\"ci_low_cassandra\"] = 0\n",
    "\n",
    "    return json_model, is_jsons_model_changed\n",
    "\n",
    "def extract_coefficients_and_confidence_intervals(summary, paid_media):\n",
    "    \"\"\"\n",
    "    Extracts coefficients and, for paid media variables, bootstrap means and confidence intervals\n",
    "    from the model's summary.\n",
    "\n",
    "    Args:\n",
    "        json_model (dict): The JSON model object containing 'ExportedModel' and its 'summary'.\n",
    "        paid_media (list): A list of strings representing the names of paid media variables.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing coefficients and, for paid media variables,\n",
    "              bootstrap means and confidence intervals.\n",
    "    \"\"\"\n",
    "    coef_dict = {}\n",
    "\n",
    "    for elem in summary:\n",
    "        # Always extract coefficients\n",
    "        coef_key = f\"{elem['variable']}_coef\"\n",
    "        coef_dict[coef_key] = elem['coef']\n",
    "\n",
    "        # For paid media variables, extract additional statistics if available\n",
    "        if elem[\"variable\"] in paid_media:\n",
    "            boot_mean_key = f\"{elem['variable']}_boot_mean_cassandra\"\n",
    "            ci_up_key = f\"{elem['variable']}_ci_up_cassandra\"\n",
    "            ci_low_key = f\"{elem['variable']}_ci_low_cassandra\"\n",
    "\n",
    "            # Check for Cassandra-specific or general bootstrapping metrics\n",
    "            if 'boot_mean_cassandra' in elem:\n",
    "                coef_dict[boot_mean_key] = elem['boot_mean_cassandra']\n",
    "                coef_dict[ci_up_key] = elem['ci_up_cassandra']\n",
    "                coef_dict[ci_low_key] = elem['ci_low_cassandra']\n",
    "            elif 'boot_mean' in elem:\n",
    "                coef_dict[boot_mean_key] = elem['boot_mean']\n",
    "                coef_dict[ci_up_key] = elem['ci_up']\n",
    "                coef_dict[ci_low_key] = elem['ci_low']\n",
    "\n",
    "    return coef_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1a79e5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_holidays=pd.read_csv(\"dataset_holidays.csv\")\n",
    "json_model=json.load(open(\"model-3_623_8.json\"))\n",
    "alldecomp=pd.read_csv(\"decomp-ms.csv\")\n",
    "df=pd.read_csv(\"dataset-ms.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "23139a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:03:14 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:03:14 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    }
   ],
   "source": [
    "ridge_model, ridge_result, df_alldecomp_matrix, df_adstock, df_saturation, summary_dict = import_model(json_model, df, df_holidays, prophet_future_dataframe_periods=14,\n",
    "    prophet_seasonality_mode='additive', ridge_size=0.2, ridge_positive=True, ridge_random_state=42, type_of_use='import')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ce6adfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fx_gradient(x, list_x, coeff, alpha, gamma, shape, scale, expected_spend_days, rollingWindowLength):\n",
    "    \"\"\"\n",
    "    Calculates the gradient of the function fx.\n",
    "\n",
    "    Parameters:\n",
    "    - x: The input value.\n",
    "    - list_x: A list of historical values.\n",
    "    - coeff: A coefficient value.\n",
    "    - alpha: An alpha value.\n",
    "    - gamma: A gamma value.\n",
    "    - shape: A shape value.\n",
    "    - scale: A scale value.\n",
    "    - expected_spend_days: The number of expected spend days.\n",
    "\n",
    "    Returns:\n",
    "    - xOut: The gradient of the function fx.\n",
    "    \"\"\"\n",
    "    x_spends = list_x.copy()\n",
    "    if len(x_spends) < 900:\n",
    "        x_spends = x_spends + ([x if not pd.isna(x) else 0] * expected_spend_days)\n",
    "    else:\n",
    "        x_spends = x_spends[-28:] + ([x if not pd.isna(x) else 0] * expected_spend_days)\n",
    "    xAdstocked = adstock(x_spends, shape, scale)['x_decayed']\n",
    "    x_to_saturation = xAdstocked[-rollingWindowLength:]\n",
    "\n",
    "    xOut = -coeff * ((alpha * (gamma**alpha) * (x_to_saturation**(alpha - 1))) / (x_to_saturation**alpha + gamma**alpha)**2)\n",
    "    return xOut\n",
    "\n",
    "def fx_objective(x, list_x, coeff, alpha, gamma, shape, scale, expected_spend_days, rollingWindowLength):\n",
    "    \"\"\"\n",
    "    Calculate the objective value for a given input.\n",
    "\n",
    "    Parameters:\n",
    "    x (float): The input value.\n",
    "    list_x (list): The list of input values.\n",
    "    coeff (float): The coefficient value.\n",
    "    alpha (float): The alpha value.\n",
    "    gamma (float): The gamma value.\n",
    "    shape (float): The shape value.\n",
    "    scale (float): The scale value.\n",
    "    expected_spend_days (int): The number of expected spend days.\n",
    "\n",
    "    Returns:\n",
    "    float: The objective value.\n",
    "    \"\"\"\n",
    "    x_spends = list_x.copy()\n",
    "    if len(x_spends) < 900:\n",
    "        x_spends = x_spends + ([x if not pd.isna(x) else 0] * expected_spend_days)\n",
    "    else:\n",
    "        x_spends = x_spends[-28:] + ([x if not pd.isna(x) else 0] * expected_spend_days)\n",
    "\n",
    "    xAdstocked = adstock(x_spends, shape, scale)['x_decayed']\n",
    "    x_to_saturation = xAdstocked[-rollingWindowLength:]\n",
    "    \n",
    "    xDecomp = [coeff * sh if not pd.isna(sh) else 0 for sh in saturation_hill(x_to_saturation, alpha, gamma)]\n",
    "    xOut = np.mean(xDecomp[-expected_spend_days:])\n",
    "\n",
    "    return xOut\n",
    "\n",
    "def budget_allocation(spends, lb, ub, expected_spend_days, maxeval, xtol_rel, eval_list, df_list, rollingWindowLength, target_value=None, target_var_type=None):\n",
    "    \"\"\"\n",
    "    Perform budget allocation optimization using the COBYLA algorithm.\n",
    "\n",
    "    Args:\n",
    "        spends (list): List of initial budget spends.\n",
    "        lb (list): List of lower bounds for budget spends.\n",
    "        ub (list): List of upper bounds for budget spends.\n",
    "        expected_spend_days (int): Expected number of spend days.\n",
    "        maxeval (int): Maximum number of function evaluations.\n",
    "        xtol_rel (float): Relative tolerance for convergence.\n",
    "        eval_list (dict): Dictionary containing evaluation data.\n",
    "        df_list (list): List of dataframes for evaluation.\n",
    "        target_value (float, optional): Target value for optimization. Defaults to None.\n",
    "        target_var_type (str, optional): Type of target variable. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        list: Optimized budget spends.\n",
    "    \"\"\"\n",
    "    \n",
    "    expSpendUnitTotal = eval_list[\"expSpendUnitTotal\"]\n",
    "\n",
    "    def eval_f(_spends, grad):\n",
    "        \"\"\"\n",
    "        Evaluate the objective function for optimization.\n",
    "\n",
    "        Parameters:\n",
    "        _spends (list): A list of spend values.\n",
    "        grad (numpy.ndarray): The gradient array.\n",
    "\n",
    "        Returns:\n",
    "        float: The objective function value.\n",
    "\n",
    "        \"\"\"\n",
    "        X = _spends.copy()\n",
    "        coefsFiltered = eval_list[\"coefsFiltered\"]\n",
    "        alphas = eval_list[\"alphas\"]\n",
    "        shapes = eval_list[\"shapes\"]\n",
    "        scales = eval_list[\"scales\"]\n",
    "        gammas = eval_list[\"gammas\"]\n",
    "        \n",
    "        if grad.size > 0:\n",
    "            # Calculate the gradient only if it's needed (i.e., if the 'grad' array is not empty)\n",
    "            grad = [fx_gradient(x, list_x, coeff, alpha, gamma, shape, scale, expected_spend_days, rollingWindowLength) for x, list_x, coeff, alpha, gamma, shape, scale in zip(X, df_list, coefsFiltered, alphas, gammas, shapes, scales)]\n",
    "\n",
    "        if target_value is None:\n",
    "            total_spend = sum(X)\n",
    "            total_response = -sum(([fx_objective(x, list_x, coeff, alpha, gamma, shape, scale, expected_spend_days, rollingWindowLength) for x, list_x, coeff, alpha, gamma, shape, scale in zip(X, df_list, coefsFiltered, alphas, gammas, shapes, scales)]))\n",
    "\n",
    "            return total_response\n",
    "        else:\n",
    "            total_response = sum(([fx_objective(x, list_x, coeff, alpha, gamma, shape, scale, expected_spend_days, rollingWindowLength) for x, list_x, coeff, alpha, gamma, shape, scale in zip(X, df_list, coefsFiltered, alphas, gammas, shapes, scales)]))\n",
    "            total_spend = sum(X)\n",
    "\n",
    "            if target_var_type != 'Revenue':\n",
    "                objective_value = round(total_spend / total_response, 2)\n",
    "            else:\n",
    "                objective_value = round(total_response / total_spend, 2)\n",
    "\n",
    "            return abs(float(target_value) - float(objective_value))\n",
    "\n",
    "    def eval_g_eq(_spends, grad, _expSpendUnitTotal):\n",
    "        X = _spends.copy()\n",
    "        total_budget_unit = _expSpendUnitTotal\n",
    "        constr = sum(X) - total_budget_unit\n",
    "        grad = np.ones(len(_spends))\n",
    "        \n",
    "        return constr\n",
    "    \n",
    "    def eval_g_eq_effi(_spends, grad, _target_value, _target_var_type):\n",
    "        \"\"\"\n",
    "        Calculate the constraint value for the optimization problem.\n",
    "\n",
    "        Args:\n",
    "            _spends (list): List of spend values.\n",
    "            grad (float): Gradient value.\n",
    "            _target_value (float): Target value.\n",
    "            _target_var_type (str): Type of target variable.\n",
    "\n",
    "        Returns:\n",
    "            float: Constraint value.\n",
    "\n",
    "        Raises:\n",
    "            None\n",
    "        \"\"\"\n",
    "        X = _spends.copy()\n",
    "        Y = float(_target_value)\n",
    "        coefsFiltered = eval_list[\"coefsFiltered\"]\n",
    "        alphas = eval_list[\"alphas\"]\n",
    "        shapes = eval_list[\"shapes\"]\n",
    "        scales = eval_list[\"scales\"]\n",
    "        gammas = eval_list[\"gammas\"]\n",
    "        \n",
    "        total_response = sum(([fx_objective(x, list_x, coeff, alpha, gamma, shape, scale, expected_spend_days, rollingWindowLength) for x, list_x, coeff, alpha, gamma, shape, scale in zip(X, df_list, coefsFiltered, alphas, gammas, shapes, scales)]))\n",
    "        total_spend = sum(X)\n",
    "\n",
    "        if _target_var_type != 'Revenue':\n",
    "            constr = total_spend - total_response * Y\n",
    "        else:\n",
    "            constr = total_spend - total_response / Y\n",
    "\n",
    "        return abs(constr)\n",
    "    \n",
    "    opt = nlopt.opt(nlopt.LN_COBYLA, len(spends))\n",
    "    \n",
    "    if target_value is None:\n",
    "        lambda_constraint = lambda x, grad: eval_g_eq(x, grad, expSpendUnitTotal)\n",
    "        opt.add_inequality_constraint(lambda_constraint, xtol_rel)\n",
    "        opt.add_equality_constraint(lambda_constraint, xtol_rel)\n",
    "    else:  \n",
    "        lambda_constraint = lambda x, grad: eval_g_eq_effi(x, grad, target_value, target_var_type)\n",
    "        #opt.add_inequality_constraint(lambda_constraint)\n",
    "        opt.add_equality_constraint(lambda_constraint, xtol_rel)\n",
    "\n",
    "    opt.set_lower_bounds(lb)\n",
    "    opt.set_upper_bounds(ub)\n",
    "\n",
    "    opt.set_xtol_rel(xtol_rel)\n",
    "    opt.set_maxeval(maxeval)\n",
    "\n",
    "    opt.set_min_objective(eval_f)\n",
    "        \n",
    "    optmSpendUnit = opt.optimize(spends)\n",
    "\n",
    "    return optmSpendUnit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "084a00f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def find_optimized_spend():\n",
    "    exp_spend_unit_total = 18124\n",
    "    init_spend_unit = [0.0, 496.955, 275.7, 0.0, 0.0, 540.9200000000001, 32.294999999999995, 947.98, 2014.315, 223.0, 0.0, 0.0]\n",
    "    period_to_estimate = 4\n",
    "    coefs = extract_coefficients_and_confidence_intervals(json_model['ExportedModel']['summary'], json_model['InputCollect']['paid_media_vars'])\n",
    "    hypers, lambda_value = extract_hyperparameters(json_model['ExportedModel']['hyper_values'])\n",
    "    summary_dict = create_summary_dictionary(json_model['InputCollect']['all_ind_vars'], hypers, coefs)\n",
    "\n",
    "    channel_constr_low_sorted = [0, 367.3598589405508, 214.83203998480366, 0, 0, 387.9317747252748, 23.420789163628577, 801.7245691391942, 1547.317561793812, 168.19980933924222, 0, 0]\n",
    "    channel_constr_up_sorted = [175.12832298136647, 626.5501410594492, 336.5679600151963, 0, 197.54807453416151, 693.9082252747254, 41.16921083637141, 1094.235430860806, 2481.3124382061883, 277.8001906607578, 377.5833333333333, 40.46204968944099]\n",
    "\n",
    "    channelConstrMeanSorted = [(low + up) / 2 for low, up in zip(channel_constr_low_sorted, channel_constr_up_sorted)]\n",
    "\n",
    "    paid_media_spends = [channelConstrMeanSorted[i] if spend < channel_constr_low_sorted[i] or spend > channel_constr_up_sorted[i] else spend for i, spend in enumerate(init_spend_unit)]\n",
    "\n",
    "    eval_list = {\n",
    "        'coefsFiltered': [summary_dict[col]['coef'] for col in json_model['InputCollect']['paid_media_vars']],\n",
    "        'alphas': [summary_dict[col]['alphas'] for col in json_model['InputCollect']['paid_media_vars']],\n",
    "        'gammas': [summary_dict[col]['gammas'] for col in json_model['InputCollect']['paid_media_vars']],\n",
    "        'shapes': [summary_dict[col]['shapes'] for col in json_model['InputCollect']['paid_media_vars']],\n",
    "        'scales': [summary_dict[col]['scales'] for col in json_model['InputCollect']['paid_media_vars']],\n",
    "        'expSpendUnitTotal': exp_spend_unit_total\n",
    "    }\n",
    "\n",
    "    paid_media_spend_traspose = df[json_model['InputCollect']['paid_media_vars']].transpose().values.tolist()\n",
    "\n",
    "    target_value = None\n",
    "\n",
    "    budget_allocator_spend = budget_allocation(paid_media_spends, channel_constr_low_sorted, channel_constr_up_sorted, period_to_estimate, 700, 1e-10, eval_list, paid_media_spend_traspose, period_to_estimate, target_value, 'R')\n",
    "\n",
    "    return budget_allocator_spend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3e06ee8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c3/0cwxpb393tj3x1r37b8gtjn80000gn/T/ipykernel_2148/3614563316.py:1044: RuntimeWarning: invalid value encountered in divide\n",
      "  x_scurve = x**alpha / (x**alpha + inflexion**alpha) # plot(x_scurve) summary(x_scurve)\n"
     ]
    }
   ],
   "source": [
    "budget_allocator_spend = find_optimized_spend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "12219581",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 175.12832298,  626.55014106,  336.56796002,    0.        ,\n",
       "        197.54807453,  693.90822527,   41.16921084, 1094.23543086,\n",
       "       2481.31243821,  277.80019066,  377.58333333,   40.46204969])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "budget_allocator_spend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5447f700",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc29aa45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
